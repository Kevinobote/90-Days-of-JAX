{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d929b5ef",
   "metadata": {},
   "source": [
    "# Learning JAX\n",
    "\n",
    "JAX is a Python library developed by Google Research for high-performance numerical computing, especially well-suited for machine learning research. It combines the familiar NumPy API with powerful transformations like automatic differentiation, JIT compilation, and vectorization, enabling you to write highly efficient code that can run on CPUs, GPUs, and TPUs.\n",
    "\n",
    "\n",
    "This document provides a quick overview of essential JAX features, so you can get started with JAX quickly:\n",
    "\n",
    "- JAX provides a unified NumPy-like interface to computations that run on CPU, GPU, or TPU, in local or distributed settings.\n",
    "\n",
    "- JAX features built-in Just-In-Time (JIT) compilation via Open XLA[https://github.com/openxla], an open-source machine learning compiler ecosystem.\n",
    "\n",
    "- JAX functions support efficient evaluation of gradients via its automatic differentiation transformations.\n",
    "\n",
    "- JAX functions can be automatically vectorized to efficiently map them over arrays representing batches of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe04a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d87b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"jax[cuda12]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02dfdd",
   "metadata": {},
   "source": [
    "JAX as NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9706f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c95c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.05      2.1       3.1499999 4.2      ]\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(selu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a35de",
   "metadata": {},
   "source": [
    "Just-in-time compilation with jax.jit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49823a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.18 ms ± 150 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(1701)\n",
    "x = random.normal(key, (1_000_000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840830d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 μs ± 51.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "_ = selu_jit(x)  # compiles on first call\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e246c75",
   "metadata": {},
   "source": [
    "Taking derivatives with jax.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6496c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661197 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "\n",
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56aa8cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1964569  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x, eps=1E-3):\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f146d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0353256\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2c3d6",
   "metadata": {},
   "source": [
    "jax.jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be4efa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        2.7182817 0.       ]\n",
      " [0.        0.        7.389056 ]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacobian\n",
    "print(jacobian(jnp.exp)(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedb68f",
   "metadata": {},
   "source": [
    "For more advanced autodiff operations, you can use jax.vjp() for reverse-mode vector-Jacobian products, and jax.jvp() and jax.linearize() for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. For example, jax.jvp() and jax.vjp() are used to define the forward-mode jax.jacfwd() and reverse-mode jax.jacrev() for computing Jacobians in forward- and reverse-mode, respectively. Here’s one way to compose them to make a function that efficiently computes full Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab99ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.         -0.         -0.        ]\n",
      " [-0.         -0.09085776 -0.        ]\n",
      " [-0.         -0.         -0.07996249]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))\n",
    "print(hessian(sum_logistic)(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280222f",
   "metadata": {},
   "source": [
    "## Auto-vectorization with jax.vmap()\n",
    "Another useful transformation is vmap(), the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of explicitly looping over function calls, it transforms the function into a natively vectorized version for better performance. When composed with jit(), it can be just as performant as manually rewriting your function to operate over an extra batch dimension.\n",
    "\n",
    "We’re going to work with a simple example, and promote matrix-vector products into matrix-matrix products using vmap(). Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "107a2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, key2 = random.split(key)\n",
    "mat = random.normal(key1, (150, 100))\n",
    "batched_x = random.normal(key2, (10, 100))\n",
    "\n",
    "def apply_matrix(x):\n",
    "  return jnp.dot(mat, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b370a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "579 μs ± 17.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d5fb93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "16.7 μs ± 2.18 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "@jit\n",
    "def batched_apply_matrix(batched_x):\n",
    "  return jnp.dot(batched_x, mat.T)\n",
    "\n",
    "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
    "                           batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fde8cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "20.9 μs ± 2.15 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap\n",
    "\n",
    "@jit\n",
    "def vmap_batched_apply_matrix(batched_x):\n",
    "  return vmap(apply_matrix)(batched_x)\n",
    "\n",
    "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
    "                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796b56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
