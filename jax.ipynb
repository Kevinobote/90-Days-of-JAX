{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning JAX: A Beginner's Guide\n",
    "\n",
    "Welcome to JAX! JAX is a Python library for high-performance numerical computing, especially well-suited for machine learning research. It combines a familiar NumPy-like API with powerful transformations like Just-In-Time (JIT) compilation, automatic differentiation, and automatic vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before we start, you need to install JAX. The installation command depends on whether you want to use JAX with CPU, GPU, or TPU support.\n",
    "\n",
    "**For CPU-only:**\n",
    "```bash\n",
    "pip install --upgrade \"jax[cpu]\"\n",
    "```\n",
    "\n",
    "**For NVIDIA GPU:**\n",
    "You'll need to have CUDA and cuDNN installed. Then, find the appropriate JAX wheel for your CUDA version from the [official JAX installation guide](https://github.com/google/jax#installation).\n",
    "An example command (replace with the correct one for your setup):\n",
    "```bash\n",
    "pip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "```\n",
    "\n",
    "**For Google Cloud TPU:**\n",
    "JAX is often pre-installed or easily installable in TPU environments.\n",
    "```bash\n",
    "pip install --upgrade jax jaxlib -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "```\n",
    "\n",
    "Let's run the CPU installation command in this notebook (you can comment it out if you've already installed it or need a different version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade \"jax[cpu]\" # Run this if you haven't installed JAX yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart: Key Concepts\n",
    "\n",
    "Let's dive into the core ideas that make JAX unique and powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. JAX NumPy (`jax.numpy`)\n",
    "\n",
    "JAX provides a NumPy-compatible API through `jax.numpy`, which is conventionally imported as `jnp`.\n",
    "If you know NumPy, you're already halfway there! Most NumPy functions have a `jnp` equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX array: [1. 2. 3.], type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "NumPy array: [1. 2. 3.], type: <class 'numpy.ndarray'>\n",
      "Sum of JAX arrays: [2. 4. 6.]\n",
      "Dot product of JAX arrays: 14.0\n",
      "Device of x_jnp: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np # We'll use standard NumPy for comparison sometimes\n",
    "\n",
    "# Create a JAX array\n",
    "x_jnp = jnp.array([1.0, 2.0, 3.0])\n",
    "print(f\"JAX array: {x_jnp}, type: {type(x_jnp)}\")\n",
    "\n",
    "# Create a NumPy array for comparison\n",
    "x_np = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"NumPy array: {x_np}, type: {type(x_np)}\")\n",
    "\n",
    "# Basic operations look the same\n",
    "y_jnp = jnp.arange(1, 4, dtype=jnp.float32)\n",
    "sum_jnp = x_jnp + y_jnp\n",
    "print(f\"Sum of JAX arrays: {sum_jnp}\")\n",
    "\n",
    "prod_jnp = jnp.dot(x_jnp, y_jnp)\n",
    "print(f\"Dot product of JAX arrays: {prod_jnp}\")\n",
    "\n",
    "# JAX arrays are typically on a device (CPU by default here, could be GPU/TPU)\n",
    "print(f\"Device of x_jnp: {x_jnp.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immutability\n",
    "A key difference from NumPy is that JAX arrays are **immutable**. This means once a JAX array is created, its contents cannot be changed in-place. Operations that seem to modify an array actually return a *new* array.\n",
    "This is crucial for JAX's functional programming paradigm and enables its powerful transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: [1 2 3]\n",
      "Original x after trying to update (it's unchanged!): [1 2 3]\n",
      "New array y with the update: [10  2  3]\n",
      "New array z (x[1]+5): [1 7 3]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 3])\n",
    "print(f\"Original x: {x}\")\n",
    "\n",
    "# This would raise an error in JAX:\n",
    "# x[0] = 10 \n",
    "\n",
    "# To update an array, you use functional methods like .at[].set()\n",
    "y = x.at[0].set(10)\n",
    "print(f\"Original x after trying to update (it's unchanged!): {x}\")\n",
    "print(f\"New array y with the update: {y}\")\n",
    "\n",
    "# Other update operations:\n",
    "z = x.at[1].add(5) # Adds 5 to the element at index 1\n",
    "print(f\"New array z (x[1]+5): {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Just-In-Time (JIT) Compilation (`jax.jit`)\n",
    "\n",
    "JAX can compile your Python functions into highly optimized machine code using XLA (Accelerated Linear Algebra compiler). This is done using the `jax.jit` transformation.\n",
    "\n",
    "**Benefits:**\n",
    "- **Speed:** Compiled code often runs much faster, especially for complex computations or code inside loops.\n",
    "- **Kernel Fusion:** XLA can fuse multiple operations into a single kernel, reducing overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled function time: 0.366768 seconds\n",
      "Compiled function (first run): 0.058543 seconds\n",
      "Compiled function (second run): 0.009382 seconds\n",
      "Results are close: True\n",
      "Decorated JIT function (first run): 0.048020 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def slow_function(x):\n",
    "  # A somewhat arbitrary computation\n",
    "  return jnp.sum(jnp.sin(x) * jnp.cos(x) + jnp.tanh(x) / (jnp.exp(x) + 1e-6))\n",
    "\n",
    "large_array = jnp.arange(1_000_000, dtype=jnp.float32)\n",
    "\n",
    "# Time the uncompiled function\n",
    "# JAX execution is asynchronous. block_until_ready() ensures computation finishes for timing.\n",
    "start_time = time.time()\n",
    "result_uncompiled = slow_function(large_array).block_until_ready()\n",
    "time_uncompiled = time.time() - start_time\n",
    "print(f\"Uncompiled function time: {time_uncompiled:.6f} seconds\")\n",
    "\n",
    "# Compile the function with jax.jit\n",
    "fast_function = jax.jit(slow_function)\n",
    "\n",
    "# Time the compiled function (first run includes compilation time)\n",
    "start_time = time.time()\n",
    "result_compiled_first = fast_function(large_array).block_until_ready()\n",
    "time_compiled_first = time.time() - start_time\n",
    "print(f\"Compiled function (first run): {time_compiled_first:.6f} seconds\")\n",
    "\n",
    "# Time the compiled function (subsequent runs are faster)\n",
    "start_time = time.time()\n",
    "result_compiled_second = fast_function(large_array).block_until_ready()\n",
    "time_compiled_second = time.time() - start_time\n",
    "print(f\"Compiled function (second run): {time_compiled_second:.6f} seconds\")\n",
    "\n",
    "# Check results are the same\n",
    "print(f\"Results are close: {jnp.allclose(result_uncompiled, result_compiled_second)}\")\n",
    "\n",
    "# You can also use @jax.jit as a decorator\n",
    "@jax.jit\n",
    "def even_faster_function(x):\n",
    "  return jnp.sum(jnp.sin(x) * jnp.cos(x) + jnp.tanh(x) / (jnp.exp(x) + 1e-6))\n",
    "\n",
    "start_time = time.time()\n",
    "result_decorated = even_faster_function(large_array).block_until_ready()\n",
    "time_decorated = time.time() - start_time\n",
    "print(f\"Decorated JIT function (first run): {time_decorated:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on JIT:**\n",
    "- The first time a JIT-compiled function is called with specific input shapes and types, JAX traces and compiles it. This incurs some overhead.\n",
    "- Subsequent calls with the *same* input shapes and types will use the cached, compiled version and be much faster.\n",
    "- JIT works best with *pure functions*: functions whose output depends only on their inputs and have no side effects (like printing or modifying global variables within the traced part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Automatic Differentiation (`jax.grad`)\n",
    "\n",
    "JAX can automatically compute gradients (and higher-order derivatives) of your functions. This is the backbone of modern machine learning.\n",
    "\n",
    "- `jax.grad(fun)`: Returns a new function that computes the gradient of `fun` with respect to its first argument.\n",
    "- `argnums` parameter: To specify differentiation with respect to other arguments.\n",
    "- `jax.value_and_grad(fun)`: Returns a function that computes both the value of `fun` and its gradient simultaneously, which can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 8.0\n",
      "f'(2.0) = 12.0\n",
      "dg/dx(2.0, 3.0) = 12.0\n",
      "dg/dy(2.0, 3.0) = 4.0\n",
      "(dg/dx, dg/dy) at (2.0, 3.0) = (Array(12., dtype=float32, weak_type=True), Array(4., dtype=float32, weak_type=True))\n",
      "Value of f(2.0): 8.0, Gradient of f(2.0): 12.0\n",
      "Gradient of JITted f'(2.0) = 12.0\n"
     ]
    }
   ],
   "source": [
    "# A simple scalar function: f(x) = x^3\n",
    "def f(x):\n",
    "  return x**3\n",
    "\n",
    "# Get the gradient function (derivative)\n",
    "grad_f = jax.grad(f)\n",
    "\n",
    "x_val = 2.0\n",
    "print(f\"f({x_val}) = {f(x_val)}\")\n",
    "# Analytically, f'(x) = 3x^2, so f'(2.0) = 3 * (2.0)^2 = 12.0\n",
    "print(f\"f'({x_val}) = {grad_f(x_val)}\")\n",
    "\n",
    "# Function with multiple arguments: g(x, y) = x^2 * y\n",
    "def g(x, y):\n",
    "  return x**2 * y\n",
    "\n",
    "# Gradient with respect to the first argument (x) by default\n",
    "grad_g_wrt_x = jax.grad(g) # same as jax.grad(g, argnums=0)\n",
    "# Analytically, dg/dx = 2xy\n",
    "print(f\"dg/dx(2.0, 3.0) = {grad_g_wrt_x(2.0, 3.0)}\") # Expected: 2 * 2.0 * 3.0 = 12.0\n",
    "\n",
    "# Gradient with respect to the second argument (y)\n",
    "grad_g_wrt_y = jax.grad(g, argnums=1)\n",
    "# Analytically, dg/dy = x^2\n",
    "print(f\"dg/dy(2.0, 3.0) = {grad_g_wrt_y(2.0, 3.0)}\") # Expected: (2.0)^2 = 4.0\n",
    "\n",
    "# Gradients with respect to multiple arguments (returns a tuple of gradients)\n",
    "grad_g_wrt_xy = jax.grad(g, argnums=(0, 1))\n",
    "gradients_xy = grad_g_wrt_xy(2.0, 3.0)\n",
    "print(f\"(dg/dx, dg/dy) at (2.0, 3.0) = {gradients_xy}\")\n",
    "\n",
    "# Compute value and gradient together\n",
    "value_and_grad_f = jax.value_and_grad(f)\n",
    "val, grad_val = value_and_grad_f(x_val)\n",
    "print(f\"Value of f({x_val}): {val}, Gradient of f({x_val}): {grad_val}\")\n",
    "\n",
    "# Gradients work with JIT too!\n",
    "@jax.jit\n",
    "def f_jit(x):\n",
    "    return x**3\n",
    "\n",
    "grad_f_jit = jax.grad(f_jit)\n",
    "print(f\"Gradient of JITted f'({x_val}) = {grad_f_jit(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Automatic Vectorization (`jax.vmap`)\n",
    "\n",
    "`jax.vmap` is a transformation for automatically vectorizing functions. If you have a function that operates on a single data point, `vmap` can transform it into a function that efficiently operates on a batch (or multiple axes) of data points, without you needing to write explicit loops.\n",
    "\n",
    "**Benefits:**\n",
    "- **Efficiency:** Pushes looping logic to XLA for parallel execution.\n",
    "- **Simplicity:** Write code for a single instance, `vmap` handles batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single scaled_dot_product: 1.0\n",
      "Batched result using vmap: [1. 5.]\n",
      "Manual first element: 1.0\n",
      "Batched result (all args batched): [ 1. 20.]\n"
     ]
    }
   ],
   "source": [
    "# A function that operates on single vectors (e.g., a scaled dot product)\n",
    "def scaled_dot_product(a, b, scale):\n",
    "  return jnp.dot(a, b) * scale\n",
    "\n",
    "vec1 = jnp.array([1., 2., 3.])\n",
    "vec2 = jnp.array([0., 1., 0.])\n",
    "s = 0.5\n",
    "\n",
    "print(f\"Single scaled_dot_product: {scaled_dot_product(vec1, vec2, s)}\")\n",
    "\n",
    "# Now, suppose we have batches of vectors 'a' and 'b', but a single 'scale'\n",
    "batch_vec1 = jnp.array([[1., 2., 3.], [4., 5., 6.]]) # Shape (2, 3)\n",
    "batch_vec2 = jnp.array([[0., 1., 0.], [1., 0., 1.]]) # Shape (2, 3)\n",
    "\n",
    "# We want to apply scaled_dot_product element-wise to the batches.\n",
    "# `in_axes=(0, 0, None)` means:\n",
    "# - Map over the 0-th axis of the first argument (batch_vec1)\n",
    "# - Map over the 0-th axis of the second argument (batch_vec2)\n",
    "# - The third argument (scale) is broadcasted (not mapped over, treated as fixed)\n",
    "batched_scaled_dot_product = jax.vmap(scaled_dot_product, in_axes=(0, 0, None))\n",
    "\n",
    "result_vmap = batched_scaled_dot_product(batch_vec1, batch_vec2, s)\n",
    "print(f\"Batched result using vmap: {result_vmap}\")\n",
    "\n",
    "# Let's verify manually for the first element:\n",
    "manual_first = scaled_dot_product(batch_vec1[0], batch_vec2[0], s)\n",
    "print(f\"Manual first element: {manual_first}\")\n",
    "assert jnp.allclose(result_vmap[0], manual_first)\n",
    "\n",
    "# If all arguments should be batched along their first axis:\n",
    "batch_scales = jnp.array([0.5, 2.0])\n",
    "batched_all_args = jax.vmap(scaled_dot_product, in_axes=(0, 0, 0)) # or just in_axes=0\n",
    "result_vmap_all = batched_all_args(batch_vec1, batch_vec2, batch_scales)\n",
    "print(f\"Batched result (all args batched): {result_vmap_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pseudorandom Numbers (`jax.random`)\n",
    "\n",
    "JAX handles random numbers differently from NumPy to ensure reproducibility in its functional and parallel execution model. You must explicitly manage **PRNG keys**.\n",
    "\n",
    "1.  Create a master key: `key = jax.random.PRNGKey(seed)`\n",
    "2.  Split keys for use: `key, subkey = jax.random.split(key)`\n",
    "    - Each time you need random numbers for an operation, you use a `subkey`.\n",
    "    - The original `key` is updated (or a new one is returned) to be split further for future operations.\n",
    "    - This ensures that sequences of random numbers are independent if generated from different subkeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial key: [0 0]\n",
      "\n",
      "Subkey1: [ 928981903 3453687069]\n",
      "Random matrix (using subkey1):\n",
      "[[-2.4424558  -2.0356805 ]\n",
      " [ 0.20554423 -0.3535502 ]]\n",
      "Key after first split: [1797259609 2579123966]\n",
      "\n",
      "Subkey2: [1353695780 2116000888]\n",
      "Another random vector (using subkey2): [0.10429037 0.34398758 0.13106728]\n",
      "Key after second split: [4165894930  804218099]\n",
      "\n",
      "Random matrix again (using same subkey1):\n",
      "[[-2.4424558  -2.0356805 ]\n",
      " [ 0.20554423 -0.3535502 ]]\n",
      "\n",
      "Generated data from function:\n",
      "[[ 0.5087175  -0.14265323  0.48834768]\n",
      " [ 0.7908833   0.6513083   0.9378768 ]]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0) # Create a master key with a seed\n",
    "print(f\"Initial key: {key}\")\n",
    "\n",
    "# Split the key to generate random numbers for one operation\n",
    "key, subkey1 = jax.random.split(key)\n",
    "random_matrix = jax.random.normal(subkey1, (2, 2))\n",
    "print(f\"\\nSubkey1: {subkey1}\")\n",
    "print(f\"Random matrix (using subkey1):\\n{random_matrix}\")\n",
    "print(f\"Key after first split: {key}\")\n",
    "\n",
    "# Split the key again for another independent random operation\n",
    "key, subkey2 = jax.random.split(key) \n",
    "another_random_vector = jax.random.uniform(subkey2, (3,))\n",
    "print(f\"\\nSubkey2: {subkey2}\")\n",
    "print(f\"Another random vector (using subkey2): {another_random_vector}\")\n",
    "print(f\"Key after second split: {key}\")\n",
    "\n",
    "# If you use the same subkey, you get the same random numbers\n",
    "random_matrix_again = jax.random.normal(subkey1, (2, 2))\n",
    "print(f\"\\nRandom matrix again (using same subkey1):\\n{random_matrix_again}\")\n",
    "assert jnp.allclose(random_matrix, random_matrix_again)\n",
    "\n",
    "# Common pattern in functions:\n",
    "def generate_random_data(key, shape):\n",
    "  key_data, key_noise = jax.random.split(key)\n",
    "  clean_data = jax.random.uniform(key_data, shape)\n",
    "  noise = jax.random.normal(key_noise, shape) * 0.1\n",
    "  return clean_data + noise\n",
    "\n",
    "key, subkey_for_func = jax.random.split(key)\n",
    "my_data = generate_random_data(subkey_for_func, (2,3))\n",
    "print(f\"\\nGenerated data from function:\\n{my_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Key Transformations\n",
    "\n",
    "- **`jax.numpy` (`jnp`)**: Your familiar NumPy API, but for JAX arrays (which are immutable).\n",
    "- **`jax.jit`**: Compiles your Python functions (using `jnp`) into fast XLA code.\n",
    "- **`jax.grad`**: Computes gradients of your functions.\n",
    "- **`jax.vmap`**: Automatically vectorizes your functions to handle batches of data.\n",
    "- **`jax.random`**: Explicit PRNG key management for reproducible random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Further Learning\n",
    "\n",
    "This notebook covered the very basics to get you started. JAX has many more powerful features. Here are some topics you might want to explore next from the list you provided or the official JAX documentation:\n",
    "\n",
    "- **Working with Pytrees:** JAX functions often operate on nested structures of arrays (like lists, tuples, dicts of arrays). Understanding pytrees is key for more complex models.\n",
    "- **Control Flow (`jax.lax`):** For using `if/else` or loops within JIT-compiled functions (e.g., `jax.lax.cond`, `jax.lax.scan`).\n",
    "- **Parallel Programming (`jax.pmap`):** For distributing computations across multiple devices (e.g., multiple GPUs or TPU cores).\n",
    "- **Stateful Computations:** How to manage state in a functional programming paradigm (often involves passing state explicitly through functions).\n",
    "- **Advanced Automatic Differentiation:** Hessians, Jacobians, custom VJPs/JVPs.\n",
    "- **Debugging in JAX:** Techniques and tools for finding issues in JAX code.\n",
    "- **JAX - The Sharp Bits 🔪:** A great resource in the JAX documentation that highlights common pitfalls and how to avoid them.\n",
    "\n",
    "The official JAX GitHub repository and its documentation are excellent resources.\n",
    "\n",
    "Happy JAXing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
